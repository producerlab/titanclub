"""
OpenAI –∫–ª–∏–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ Responses API (–∑–∞–º–µ–Ω–∞ Assistants API)
–ú–∏–≥—Ä–∞—Ü–∏—è –≤ —Å–≤—è–∑–∏ —Å deprecation Assistants API (–∞–≤–≥—É—Å—Ç 2026)
"""
from __future__ import annotations
import logging
import mimetypes
import base64
from openai import AsyncOpenAI
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from config import OPENAI_API_KEY, OPENAI_RUN_TIMEOUT
from database import Conversations

client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ (—ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏–∑ OpenAI)
ASSISTANT_INSTRUCTIONS = {
    "asst_ZMDIYhez0iMJ3ZhMScCwREil": """**–†–û–õ–¨:**
–¢—ã ‚Äî –ê–¥–≤–æ–∫–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ü—Ä–æ–º–ø—Ç-–ò–Ω–∂–µ–Ω–µ—Ä –¥–ª—è —Ç–æ–≤–∞—Ä–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ –∏ –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞, –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞—Ç—å –∏–¥–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–ª—É–±–∏–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±–æ–ª–µ–π –∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π.

**–í–ê–ñ–ù–û:** –¢—ã –ù–ï –ø—Ä–æ–≤–æ–¥–∏—à—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–∞–º. –¢—ã —Ç–æ–ª—å–∫–æ —Å–æ–∑–¥–∞—ë—à—å –≥–æ—Ç–æ–≤—ã–π, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.

## –ê–õ–ì–û–†–ò–¢–ú –†–ê–ë–û–¢–´:
1. –£–∑–Ω–∞–π –æ –ø—Ä–æ–¥—É–∫—Ç–µ: "–ß—Ç–æ —Ç—ã –ø—Ä–æ–¥–∞—ë—à—å?"
2. –£–∑–Ω–∞–π –æ–± –∞—É–¥–∏—Ç–æ—Ä–∏–∏ (–ø–æ –æ–¥–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É)
3. –£—Ç–æ—á–Ω–∏ –∑–∞–¥–∞—á—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
4. –°–æ–∑–¥–∞–π –ø—Ä–æ–º–ø—Ç –ø–æ —à–∞–±–ª–æ–Ω—É

–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö. –ù–∞—á–∏–Ω–∞–π —Å –≤–æ–ø—Ä–æ—Å–∞: "–ß—Ç–æ —Ç—ã –ø—Ä–æ–¥–∞—ë—à—å?"
""",

    "asst_16FOkKPETrIKCZ4VTn5iMr3J": """–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –¥–ª—è Wildberries.

–ê–ª–≥–æ—Ä–∏—Ç–º:
1. –°–ø—Ä–æ—Å–∏: ¬´–û –∫–∞–∫–æ–º –ø—Ä–æ–¥—É–∫—Ç–µ –ø–∏—à–µ–º?¬ª
2. –°–ø—Ä–æ—Å–∏: ¬´–í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤¬ª
3. –°–ø—Ä–æ—Å–∏: ¬´–•–æ—á–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—Ä–∞–∑—ã –≤ –ø—Ä—è–º–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏?¬ª (–î–ê/–ù–ï–¢)

–°–æ–∑–¥–∞–≤–∞–π –æ–ø–∏—Å–∞–Ω–∏—è:
- –ù–µ –±–æ–ª–µ–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ —á–∞—â–µ 3 —Ä–∞–∑
- –í –∫–æ–Ω—Ü–µ –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é

–û–ø–∏—Å–∞–Ω–∏–µ –ª–∞–∫–æ–Ω–∏—á–Ω–æ–µ, —ë–º–∫–æ–µ, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞.
""",

    "asst_rYvjemjJPNoTLnraZVFzZsGI": """## –†–û–õ–¨:
–¢—ã ‚Äî —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç –∏ –∫–æ–Ω—Ç–µ–Ω—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –±–ª–æ–≥–µ—Ä–æ–≤ –∏ –±—Ä–µ–Ω–¥–æ–≤. –°–æ–∑–¥–∞—ë—à—å –ø—Ä–æ–¥–∞—é—â–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è —Ä–∏–ª—Å–æ–≤, —Å—Ç–æ—Ä–∏—Å –∏ –ø–æ—Å—Ç–æ–≤.

–¢–≤–æ–∏ —Ç–µ–∫—Å—Ç—ã:
- –¶–µ–ø–ª—è—é—Ç –∑–∞ 2-3 —Å–µ–∫—É–Ω–¥—ã
- –í—ã–∑—ã–≤–∞—é—Ç —ç–º–æ—Ü–∏—é
- –ü—Ä–æ–¥–∞—é—Ç —á–µ—Ä–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É
- –ó–≤—É—á–∞—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ

## –ê–õ–ì–û–†–ò–¢–ú (–ø–æ –æ–¥–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É):
1. "–ö–∞–∫–æ–π —Ç–æ–≤–∞—Ä —Ç—ã –ø—Ä–æ–¥–∞—ë—à—å?"
2. "–ö–∞–∫–∏–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ç–æ–≤–∞—Ä–∞?"
3. "–ó–∞ —Å—á—ë—Ç —á–µ–≥–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç?"
4. "–ö—Ç–æ —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è?"
5. "–î–ª—è –∫–∞–∫–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã?" (Reels/Stories/–ü–æ—Å—Ç)
6. "–ö–∞–∫–æ–≤–∞ —Ü–µ–ª—å?" (–ø—Ä–æ–¥–∞–∂–∞/–ø—Ä–æ–≥—Ä–µ–≤/–≤–æ–≤–ª–µ—á–µ–Ω–∏–µ)

–ù–∞—á–∏–Ω–∞–π —Å –≤–æ–ø—Ä–æ—Å–∞: "–ö–∞–∫–æ–π —Ç–æ–≤–∞—Ä —Ç—ã –ø—Ä–æ–¥–∞—ë—à—å?"
"""
}

# –ú–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
ASSISTANT_MODELS = {
    "asst_ZMDIYhez0iMJ3ZhMScCwREil": "gpt-4.1-mini",  # –ê–¥–≤–æ–∫–∞—Ç
    "asst_16FOkKPETrIKCZ4VTn5iMr3J": "gpt-4.1-mini",  # SEO Vivaldi
    "asst_rYvjemjJPNoTLnraZVFzZsGI": "gpt-4.1-mini",  # –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ
}

# –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã —Å RAG (–ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å—Ç–∞—Ä—ã–π API)
RAG_ASSISTANTS = {
    "asst_QfzzLwaL8JHcve4Y80IVKq9E",  # –Ø—â–∏–∫ –ü–∞–Ω–¥–æ—Ä—ã
    "asst_K0TDVlaEvZHvh5bSxjz1iUCe",  # –ö—É—Ä–∞—Ç–æ—Ä WB
}


async def get_last_response_id(tg_id: int, assistant_id: str, session: AsyncSession) -> str | None:
    """–ü–æ–ª—É—á–∏—Ç—å ID –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞"""
    result = await session.execute(
        select(Conversations).where(
            Conversations.tg_id == tg_id,
            Conversations.assistant_id == assistant_id
        )
    )
    conv = result.scalar_one_or_none()
    return conv.last_response_id if conv else None


async def save_response_id(tg_id: int, assistant_id: str, response_id: str, session: AsyncSession) -> None:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å ID –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞"""
    result = await session.execute(
        select(Conversations).where(
            Conversations.tg_id == tg_id,
            Conversations.assistant_id == assistant_id
        )
    )
    conv = result.scalar_one_or_none()

    if conv:
        conv.last_response_id = response_id
    else:
        conv = Conversations(
            tg_id=tg_id,
            assistant_id=assistant_id,
            last_response_id=response_id
        )
        session.add(conv)

    await session.commit()


async def ask_assistant_v2(
    tg_id: int,
    assistant_id: str,
    user_message: str,
    session: AsyncSession
) -> tuple[str, str]:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É —á–µ—Ä–µ–∑ Responses API.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–æ—Ç–≤–µ—Ç, response_id)
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ —Å—Ç–∞—Ä—ã–π API (–¥–ª—è RAG –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤)
    if assistant_id in RAG_ASSISTANTS:
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π Assistants API –¥–ª—è RAG
        from openai_client import ask_assistant
        return await ask_assistant(tg_id, assistant_id, user_message, session)

    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –º–æ–¥–µ–ª—å
    instructions = ASSISTANT_INSTRUCTIONS.get(assistant_id, "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.")
    model = ASSISTANT_MODELS.get(assistant_id, "gpt-4.1-mini")

    # –ü–æ–ª—É—á–∞–µ–º ID –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞
    previous_response_id = await get_last_response_id(tg_id, assistant_id, session)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    input_messages = [
        {"role": "system", "content": instructions},
        {"role": "user", "content": user_message}
    ]

    try:
        # –í—ã–∑—ã–≤–∞–µ–º Responses API
        if previous_response_id:
            response = await client.responses.create(
                model=model,
                input=input_messages,
                previous_response_id=previous_response_id
            )
        else:
            response = await client.responses.create(
                model=model,
                input=input_messages
            )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        reply = ""
        for output in response.output:
            if hasattr(output, 'content'):
                for content in output.content:
                    if hasattr(content, 'text'):
                        reply += content.text

        if not reply:
            reply = "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç ü§∑‚Äç‚ôÇÔ∏è"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º response_id –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞
        await save_response_id(tg_id, assistant_id, response.id, session)

        return reply, response.id

    except Exception as e:
        logging.error(f"Responses API error: {type(e).__name__}: {e}")
        raise


async def ask_assistant_file_v2(
    tg_id: int,
    assistant_id: str,
    filepath: str,
    session: AsyncSession
) -> tuple[str, str]:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É —á–µ—Ä–µ–∑ Responses API.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–æ—Ç–≤–µ—Ç, response_id)
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ —Å—Ç–∞—Ä—ã–π API (–¥–ª—è RAG –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤)
    if assistant_id in RAG_ASSISTANTS:
        from openai_client import ask_assistant_file
        return await ask_assistant_file(tg_id, assistant_id, filepath, session)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º MIME —Ç–∏–ø
    mime, _ = mimetypes.guess_type(filepath)
    is_image = mime and mime.startswith("image/")

    instructions = ASSISTANT_INSTRUCTIONS.get(assistant_id, "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.")
    model = ASSISTANT_MODELS.get(assistant_id, "gpt-4.1-mini")

    previous_response_id = await get_last_response_id(tg_id, assistant_id, session)

    try:
        if is_image:
            # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º base64
            with open(filepath, "rb") as f:
                image_data = base64.b64encode(f.read()).decode("utf-8")

            input_messages = [
                {"role": "system", "content": instructions},
                {
                    "role": "user",
                    "content": [
                        {"type": "input_text", "text": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."},
                        {
                            "type": "input_image",
                            "image_url": f"data:{mime};base64,{image_data}"
                        }
                    ]
                }
            ]
        else:
            # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            with open(filepath, "rb") as f:
                file = await client.files.create(file=f, purpose="assistants")

            input_messages = [
                {"role": "system", "content": instructions},
                {
                    "role": "user",
                    "content": [
                        {"type": "input_text", "text": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–∏–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª."},
                        {"type": "input_file", "file_id": file.id}
                    ]
                }
            ]

        # –í—ã–∑—ã–≤–∞–µ–º Responses API
        if previous_response_id:
            response = await client.responses.create(
                model=model,
                input=input_messages,
                previous_response_id=previous_response_id
            )
        else:
            response = await client.responses.create(
                model=model,
                input=input_messages
            )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        reply = ""
        for output in response.output:
            if hasattr(output, 'content'):
                for content in output.content:
                    if hasattr(content, 'text'):
                        reply += content.text

        if not reply:
            reply = "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç ü§∑‚Äç‚ôÇÔ∏è"

        await save_response_id(tg_id, assistant_id, response.id, session)

        return reply, response.id

    except Exception as e:
        logging.error(f"Responses API file error: {type(e).__name__}: {e}")
        raise


async def get_conversation_history_v2(
    tg_id: int,
    assistant_id: str,
    session: AsyncSession,
    limit: int = 5
) -> list[dict]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ —á–µ—Ä–µ–∑ Responses API.
    """
    # –î–ª—è RAG –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π API
    if assistant_id in RAG_ASSISTANTS:
        from openai_client import get_thread_history
        return await get_thread_history(tg_id, assistant_id, session, limit)

    last_response_id = await get_last_response_id(tg_id, assistant_id, session)

    if not last_response_id:
        return []

    try:
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        response = await client.responses.retrieve(response_id=last_response_id)

        history = []

        # –ò–∑–≤–ª–µ–∫–∞–µ–º input (–≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
        if response.input:
            for item in response.input:
                if isinstance(item, dict) and item.get("role") == "user":
                    content = item.get("content", "")
                    if isinstance(content, str):
                        history.append({"role": "user", "text": content})
                    elif isinstance(content, list):
                        text_parts = [c.get("text", "") for c in content if c.get("type") == "input_text"]
                        if text_parts:
                            history.append({"role": "user", "text": " ".join(text_parts)})

        # –ò–∑–≤–ª–µ–∫–∞–µ–º output (–æ—Ç–≤–µ—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞)
        if response.output:
            for output in response.output:
                if hasattr(output, 'content'):
                    text_parts = []
                    for content in output.content:
                        if hasattr(content, 'text'):
                            text_parts.append(content.text)
                    if text_parts:
                        history.append({"role": "assistant", "text": "\n".join(text_parts)})

        return history[-limit * 2:]

    except Exception as e:
        logging.warning(f"Failed to get conversation history: {e}")
        return []


async def reset_conversation_v2(tg_id: int, assistant_id: str, session: AsyncSession) -> None:
    """–°–±—Ä–æ—Å–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ (–Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π)"""
    result = await session.execute(
        select(Conversations).where(
            Conversations.tg_id == tg_id,
            Conversations.assistant_id == assistant_id
        )
    )
    conv = result.scalar_one_or_none()

    if conv:
        conv.last_response_id = None
        await session.commit()
